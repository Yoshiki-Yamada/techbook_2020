\section{はじめに}
近年, コンテナ型仮想化の普及もあり仮想化技術という存在を様々な場面でよく聞くようになりました. しかし, コンテナ型仮想化技術以外の他の仮想化技術というのは中々個人で扱うような技術・ソフトウェアではないこともあり, あまり知られていないことが多かったりします. また, コンテナ型仮想化技術も便利で環境構築が魔法のようなすごい存在といった漠然とした解釈の人もきっと多いはずです. そこでこの章では, 各種仮想化技術の仕組みや特徴について触れながらふわっと仮想化技術について紹介します.

\section{仮想化技術とは}
仮想化技術にはいくつか種類がありますが, この章では計算機資源を抽象化してOSなどに見せるプラットフォーム仮想化のことを指します. 仮想化することで複数の計算機資源を単一に見せたり, 単一の計算機資源を複数に見せることができます. そしてプラットフォーム仮想化を支えるための技術としてシステム仮想機械(VM, Virtual Machine)と呼ばれる計算機資源をエミュレートするソフトウェアが存在します. 仮想機械の実装はハイパーバイザを始めとしていくつか存在します. 例えば仮想専用サーバ(VPS, Virtual Private Server)のようなサービスでは図2.1にように, 物理サーバ上に仮想化OSによって複数の仮想サーバに分割してユーザに各仮想サーバを提供しています.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=50mm]{./assets/chikuwa_ITasset/vps.png}
    \caption{hogehoge}
    \label{fig:vps}
\end{figure}

\section{ハイパーバイザについて}
仮想化を実現するハイパーバイザにはベアメタル型(Type1),ホスト型(Type2)の2つに分類されます.

\section{ベアメタル型(Type1)}
ベアメタル型(Type1)は, ハードウェアの上で直接動作します. この方式ではホストOSと呼ばれるような土台になるOSが存在しないため,仮想マシンによる遅延や速度低下を防ぐことができます. そしてベアメタルハイパーバイザでは実装手法でもモノリシックカーネル型とマイクロカーネル型の2つに分類することができるほか, 仮想化のアプローチで完全仮想化と準仮想化に分類することができます.
\subsection{モノリシックカーネル型}
主にVMware ESX/ESXiなどで採用されている方式で, モノリシックという英語で「1枚岩」という意味の通り、ハイパーバイザの中にデバイスドライバが含まれています. ハイパーバイザがストレージをはじめ,ネットワークや入力デバイスといったハードウェアへのアクセスをすべてを処理します. この方法の利点はハイパーバイザとデバイスドライバが密接に連携するため,オーバーヘッドが少なく効率的です. しかしながら, ハイパーバイザの中にデバイスドライバが存在しているため,ハイパーバイザ層でデバイスドライバを用意する必要があります. そのため, ハードウェアのサポートがマイクロカーネル型と比較して少なく, 使用するハードウェアに制限がかかってしまう場合があります. また, デバイスドライバをハイパーバイザに直接組み込むため, バグや脆弱性はハイパーバイザ全体に広がってしまいます.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=50mm]{./assets/chikuwa_ITasset/monolithic.png}
    \caption{モノリシックカーネル型ベアメタルハイパーバイザ}
    \label{fig:monolithic}
\end{figure}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=50mm]{./assets/chikuwa_ITasset/monolithic.png}
    \caption{モノリシックカーネル型のハードウェアアクセス}
    \label{fig:monolithic_access}
\end{figure}
\subsection{マイクロカーネル型}
主にXenやHyper-Vで採用されている方式で,　ハイパーバイザを管理する仮想マシンと管理OSを用意します. この管理OSはLinuxはWindows Serverなど汎用OSを使用します. また, 管理OSはXenではドメイン0, Hyper-Vでは親パーティションと呼ばれています. この方式では, デバイスドライバはハイパーバイザではなく, ハイパーバイザ上の仮想マシンとして動作している管理OSのデバイスドライバを使用します. 仮想マシンからハードウェアにアクセスする時はゲストOSから仮想デバイスのインターフェースを経由してハイパーバイザから管理OSに渡されます. そして管理OSのデバイスドライバからハードウェアにアクセスします.この方法は, 汎用OSのデバイスドライバを使用することで, モノリシックカーネル型に比べてハードウェアのサポートが多く, ハードウェアの対応が柔軟であるという利点があります. 例えば, Hyper-VならWindows用のデバイスドライバを使用することができます. しかしながら, ハードウェアにアクセスする際にハイパーバイザから管理OSを経由するため, モノリシックカーネル型よりも性能が低下してしまいがちであり, 管理用の汎用OSがクラッシュした場合全てのVMがクラッシュしてしまうといった欠点が存在します.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=50mm]{./assets/chikuwa_ITasset/monolithic.png}
    \caption{マイクロカーネル型ベアメタルハイパーバイザ}
    \label{fig:microkernel}
\end{figure}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=50mm]{./assets/chikuwa_ITasset/monolithic.png}
    \caption{マイクロカーネル型のハードウェアアクセス}
    \label{fig:microkernel_access}
\end{figure}

\subsection{完全仮想化}
完全仮想化方式のハイパーバイザでは, ハードウェアの挙動をすべてエミュレートします. そのため, 何も変更も加えていないそのままのホストOSを動かすことができます. 1960年代にIBMが「トラップアンドエミュレート」とよばれる方法で完全仮想化を実装しようとしました. この方法ではゲストOSが特権がない状態(Ring3)で実行させ, 特権(Ring0)が必要な命令を実行しようとすると失敗します. その際にハイパーバイザがその失敗をトラップして原因を確認してからその命令をエミュレートすることによってゲストOSの期待する結果を返すことができ, ゲストOSにRing0以外で実行されていることを気づかせないようにすることができます. しかしながら, この手法は古典的ですべてのアーキテクチャに適用できるわけではありませんでした. 特にx86プロセッサの場合, ユーザ権限で実行できるセンシティブ命令と呼ばれる計算機資源の構成などの依存している命令が存在しているため, 実装を難しくさせていました. そこで, 「バイナリトランスレーション」と呼ばれる新しい手法が使われるようになりました. この手法では, センシティブな命令以外の命令は直接CPUで実行し, センシティブな命令はハイパーバイザで実行前に動的に他の命令に置き換えられます.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=50mm]{./assets/chikuwa_ITasset/monolithic.png}
    \caption{バイナリトランスレーション}
    \label{fig:binarytranslation}
\end{figure}
\subsection{準仮想化}
準仮想化方式のハイパーバイザでは, ハイパーバイザ上で実行するゲストOSに手を加え, 仮想環境を実現しています. この方式では, ゲストOSのカーネルが発行するハードウェアを制御するシステムコールに手を加えることでハイパーバイザと協調することで高速に動作させようとするものです. なおこのような手を加えられたシステムコールはハイパーバイザコールと呼ばれます. バイナリトランスレーションではセンシティブ命令を動的に変換していたため、性能が低下しやすい特徴がありましたが, 準仮想化では静的に変換を行い修正するため, 性能には影響はありません. しかしながら, ゲストOSに予め変更を加える必要があるため, 使用するゲストOSに制限があるという欠点があります.
\section{ホスト型(Type2)}
ホスト型(Type2)はParallels DesktopやVirtualBoxなどで採用されている方式で, LinuxやWindowsといったホストOSの上でアプリケーションとして動作します. また, ハイパーバイザをインストールする先のPCをホストOSと呼びます. 主にサーバとしての仮想化よりもユーザがmacOS上でWindowsとwindows専用ソフトウェアを使用するといったようなクライアントサイドでの用途に用いられることが多いです. この方式の利点は, ホストOSを変更することもなく, アプリケーションとしてインストールすることができるため手軽に利用することができる点です. 最近ではVargrantのようなプロビジョニングツールが登場したことにより, 手軽に開発環境として仮想環境を用意することができるようにもなりました. しかしながら,ホスト型の欠点として仮想デバイスから物理デバイスにたどり着くまでにハイパーバイザ, ホストOSを経由する必要があるため, オーバヘッドがベアメタル型に比べて大きくなってしまいます.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=50mm]{./assets/chikuwa_ITasset/monolithic.png}
    \caption{ホスト型ハイパーバイザ}
    \label{fig:hosthypervisor}
\end{figure}
\section{コンテナ型仮想化}
コンテナ型仮想化はハイパーバイザによる仮想化とは少し違う存在で, 単一のOS上に「コンテナ」と呼ばれる仮想的なユーザ空間を提供しています. ここ最近Dockerと呼ばれるコンテナランタイムの普及により注目される存在となってきました. またDocker以外にもLXC/LXDやHaconiwaといった様々なコンテナランタイムが登場しています. これらのコンテナランタイムは一見するとホスト型ハイパーバイザのようにOSの上でOSを簡単に起動しているように錯覚しがちですが, あくまでもコンテナではホストOSの一つの「プロセス」であり,リソースなどを制限したり切り分けていることで一つの小さな独立した環境を作っています. ここからはLinuxコンテナランタイムに焦点を絞り, Linuxのどのような機能を使ってコンテナというものを作り上げていくかを解説していきます.
\section{Linuxコンテナランタイムをつくり上げる技術}
\subsection{Linux namespace}
Linux namespaceはOSのリソースの分離をおこなう仕組みです. Linuxの様々なリソースには「名前空間」と呼ばれるものが存在します. この名前空間を分けてあげることであたかもそのリソースしか存在しないように見せることでリソースを共存させることができます. 今回は4つの名前空間について解説します.
\subsubsection{IPC名前空間}
IPC名前空間はプロセス間通信のリソースであるSystem V IPCオブジェクトとPOSIXメッセージキュー(Linux2.6.30以降)を分離します. IPC名前空間で分離することによって名前空間が異なるプロセスが共有する共有メモリやセマフォにアクセスすることを防ぐことができます.
\subsubsection{Mount名前空間}
Mount名前空間はファイルシステムのマウントポイントを分離することで異なる名前空間のファイルシステムにアクセスを分離することができます. 通常, 子プロセスは親プロセスと同じマウントポイントを認識します. しかし新しいMount名前空間の下では, 子プロセスは任意の変更を加えることができ,親プロセスやシステム全体のMount名前空間には影響を与えません. 例えば, 各コンテナごとにMount名前空間を分けて/var, /tmpを持たせることで独立したユーザ領域を見せることができます.

